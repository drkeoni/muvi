## Jotting down ideas

I like the vinyl-inspired audio reactive video [here](http://mattdesl.github.io/spins/).  First could do the same thing
with color and MFCCs.  Thinking about having the line change direction as it spirals around (clockwise vs. counter-clockwise).
Multiple spirals?  Particle interactions?  3D?

An array of hexagons, like lily pads.  Light up with MFCC signals.

I really like the effect where foreground images are very sharp (and photorealistic ideally) and the background is
heavily blurred (depth of focus effect).

Wrapping the musical line around a faux music box cylinder,
rotating in the middle of the screen.  Lines of "music" spin off.

Go back to a crystal simulation I programmed in England.  Have a landscape
growing from falling snow crystals or sand or water drops, where the
drops hit some of them grow and become Lindenmayer ferns.

#### 02/21/2016

I'm really interested in art from reaction-diffusion systems.
[Jonathan McCabe](https://www.flickr.com/photos/jonathanmccabe/sets) has
amazing generative art---based on various algorithms---with reaction-diffusion
systems playing an important role.

To map music to a 2D art canvas, start with the song as a trajectory
in time, MFCC coefficients, and loudness.  How do we map this trajectory
to space?  And for generative systems that require significant time to evolve a
 picture (RD systems and cellular automata), how does this interrelate with
 the motion described by
the song?

For example, we can map the MFCC coefficients (call them 12) to 12 regions on
the canvas.  As the song proceeds, pictures evolve in the region corresponding
to the dominant MFCC coefficient.

Or time can correspond to the regions and we draw different pictures at different
times corresponding to the local volume and MFCC coefficients (I've implemented
this one before).

Some of the best art from McCabe is multi-scale application of the same ideas,
which introduces another way to proceed.  Allow the current scale of the
are to change based on...time?

#### 09/26/2016

In the last seven months I've continued to tweak these ideas based on reaction-diffusion
systems and MFCC coefficients.  I like the resulting dynamic visuals, but nothing
fundamentally new has shown up in my experiments (although I only revisit these codes
once every two months or so).

I'd like to get back to the ideas that started this line of coding + music in the first place.

I became interested in writing interactive video code based on music after downloading and playing
"Alto's Adventure" while living briefly in Kenya last year.  At that time I was excited by how
relatively simple music---music that I could write and was writing at the time---came to life and became
something much more relevant when paired with visuals and a narrative.  While I've seldom written compositions
at night with a narrative in mind, I could suddenly see that the narrative could be there, could augment
the song and could make a greater whole than the sum of the parts.

Being a computer programmer---and inclined toward laziness, impatience and hubris---I immediately disregarded the notion
of creating this artwork myself, and instead started musing about what it would take to create music videos that
could tell stories which sensed and responded to the music at hand.

However, while I've now made fairly compelling videos that bewitch me every time I start up the code,
I'm still falling short of the goal that I started with.

So I'd like to spend a few words thinking about how to turn music back into a story-like narrative,
and not simple responsive visuals.

For inputs, we have the same inputs that we've been working with: loudness at time t + windowed senses of this + moments thereof,
MFCC coefficients at time t + windowed senses of this + moments thereof.  And here I think I could spend more time creating
windowed measures of how fast things are changing (MFCC or volume).

The question is how to map these inputs to visuals which invoke story associations in the viewer.  There's a natural desire
to use anthropomorphic figures here, but I've always felt like this is far too complicated and tedious to code up....even
if this is a great idea.  So I'm left with considering approaches that invoke human relatedness but can be feasibly
generated by computer code without much work.

So a first idea is something from the natural world, involving plants and maybe other natural effects therein.  Plants swaying in wind and rain,
the wind, rain, lightning controlled by music?  A white tree against a black background, leaves falling, only white vs. black,
rain and lightning in white.

How hard is it to get a 2D sprite to animate happily against other backgrounds in processing?  There's lots I could do here
with objects like ships or houses.  (and of course the background might merrily evolve with rxtn-diffusion or something similar)


